{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de finetuning d'un modèle BERT sur la classification des avis du site Allociné\n",
    "\n",
    "[Ouvrir dans Google colab](https://colab.research.google.com/github/Herve07h22/nlp-fine-tuning/blob/main/allocine.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.env/lib/python3.9/site-packages (1.18.3)\n",
      "Requirement already satisfied: transformers in ./.env/lib/python3.9/site-packages (4.16.2)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.9/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: dill in ./.env/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.env/lib/python3.9/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in ./.env/lib/python3.9/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.9/site-packages (from datasets) (1.22.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.9/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in ./.env/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in ./.env/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in ./.env/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.env/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.9/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./.env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: sacremoses in ./.env/lib/python3.9/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.9/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in ./.env/lib/python3.9/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click in ./.env/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in ./.env/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Users/herve/Documents/projets/temporaire/nlp-fine-tuning/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset allocine_dataset (/Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb)\n",
      "100%|██████████| 3/3 [00:00<00:00, 68.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (160000, 2), 'validation': (20000, 2), 'test': (20000, 2)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"allocine\")\n",
    "raw_datasets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aperçu du dataset \"Allociné\"\n",
    "\n",
    "5 commentaires positifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb/cache-fce97a8856f0e508.arrow\n",
      "Loading cached processed dataset at /Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb/cache-ce9dae5424c5b159.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Encore un film iranien. Au fil des films qui arrivent en France, cette société islamique que nous ne comprenons pas, nous livre ses petits secrets et ses incohérences. Par exemple le changement de sexe est autorisé et même remboursé par l’état, un comble pour un pays dont la femme est considérée comme impure (voir le film iranien de Mehran Tamadon) et où l’homosexualité peut conduire à la peine capitale. C’est l’histoire de deux femmes (deux actrices formidables), l’une fait le taxi pour payer la sortie de prison de son mari, l’autre n’aspire qu’à une seul chose, devenir un homme. Ces deux femmes vont se rencontrer, ne pas se comprendre pour finalement s’entraider. Un très beau film avec une très forte morale sur la condition humaine et la religion. 4 étoiles',\n",
       " 'Grâce à sa réalisation d\\'une remarquable qualité, \"April Snow\" parvient à nous envouter dès la première minute. Certes, certains pourront trouver ça un peu scolaire parfois, mais on ne peut nier l\\'efficacité de chaque plan, de chaque situation, de chaque silence. Jin-Ho Hur sait tirer au maximum parti de son histoire, et en rend chacune de ses étapes des plus saisissantes. On pourra quelque peu regretter que le film s\\'essouffle progressivement dans sa seconde moitié, mais l\\'impulsion de départ était telle qu\\'on reste à un très bon niveau. Il n\\'est pas impossible que certains s\\'y ennuieront à mourir, tant l\\'émotion passe par les silences, les non-dits, les expressions des visages. Pourtant c\\'est ce qui fait tout le charme de cet \"April Snow\" au combien rafiné et délicat.',\n",
       " \"Un ex-flic est infiltré dans une prison où des combats clandestins se déroulent. Ring of death multiplie les références aux poncifs du genre. Parmi eux, In hell, Coups pour coups, Scorpion... Le directeur de la prison imbu de sa propre personne et arrogant fait penser à Donald Sutherland dans Haute sécurité. Les combats sont violents, brutaux et sans concession. On plonge dans le mon du free fight underground pour entrevoir les plus bas instincts de l'homme. Les scènes de combat étant diffusé sur le net pour le plaisir des riches spectateurs pariant sur l'un des combattants. Malsain, voyeuriste, pervers, les adjectifs ne manquent pas pour définir ces vils enfoirés. Même si Ring of death n'a rien de bien original, il se révèle efficace et furieusement défoulant. Un excellent film d'action.\",\n",
       " \"Horsehead est une véritable surprise. Beaucoup moins film d'horreur que film d'auteur, le film de Basset bluffe par la qualité et l'originalité de ce qu'il délivre : le visuel (déférent et non pas référent à un certain cinéma d'épouvante d'exploitation des 70's) impressionne lors des scènes oniriques, des thématiques fortes (de la maternité à la transmission) traitées via le symbolisme des plans et la matière-cinéma elle-même plutôt que par les prémisses d'un scénario minimaliste (et c'est une très bonne chose), une bande son cousue main dont la modernité des pulsations technoïdes s'entrechoquent avec ces images gothiques ou giallesques à l'ADN daté. Un reproche tout de même : le film souffre de la générosité du premier film et peche parfois par excès. Une impression de boursouflement dans le montage hyper cut résiste à la fluidité de l'ensemble. Mais ne boudons pas le plaisir de ce spectacle inédit dans le paysage cinématographique français et, tant est que notre sensibilité résonne avec celle du film (c'est vrai, c'est LA condition pour profiter du voyage trippal proposé par Basset), profitons de l'opportunité de se rafraîchir le regard avec un film de genre définitivement unique.\",\n",
       " 'Plus que la controverse qui fit de ce film une victime des censeurs français pendant plus de trente ans, ce que La bataille d’Alger apporte à la grande histoire du 7ème art est un réalisme frappant dans le traitement de la reconstitution de l’opposition entre les indépendantistes algériens et les militaires. Gillo Pontecorvo ne cherche aucunement à porter un quelconque jugement sur les méthodes des deux camps, mais n’est uniquement animé que par une profonde volonté de reconstitution superbement détaillée de ces événements violents, la réussite de cet aspect documentaire est tel qu’aujourd’hui son long-métrage est devenu un sujet d’études pour les soldats américains se préparant à la guérilla urbaine en Irak. La personnalité trouble des deux personnages que sont Ali La Pointe et le Colonel Mathieu est un autre élément qui éloigne cette œuvre hors du commun du manichéisme inhérent aux habituels films de guerre. Sans conteste la seule approche valable que le cinéma ait fait de la guerre d’Algérie.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples = raw_datasets[\"train\"].shuffle(seed=42).filter(lambda example: example['label']==1)\n",
    "positive_samples['review'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 commentaires négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb/cache-fce97a8856f0e508.arrow\n",
      "Loading cached processed dataset at /Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb/cache-87a99b9c87b7cc5d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Ray Milland qui a tourné dans presque 200 films s'est amusé à en réaliser 5.Celui est est le plus connu mais pas le meilleur. Il y a certes quelques séquences réussies mais beaucoup trop qui semblent fabriquées ,les comédiens manquent d'aisance et l'acteur lui même qui sait être excellent avec Hitchcock,Lang,Wilder, et surtout Farrow (the big clock-1948) a du mal à se diriger lui même. L'ambiance est ce qu'il y a de mieux .Imaginer une guerre atomique sans la voir était un pari réussi mais la volonté de trop montrer le mauvais coté de la nature humaine est bien trop excessive et démonstrative. Le contre exemple représenté par la femme du héros finit par agacer. D'autre part,un petit peu plus et nous étions dans un film d'horreur ,ce qui n'était pas le propos .A l'évidence Milland avait d'autres ambitions mais elles ne furent pas tenues. Le film a vieilli ,ce qui n'est jamais bon ,alors qu'avec plus de talent cinématographique il pourrait être encore précurseur puisque ces événements ne se sont pas produits et font partie d'un avenir hypothètique pessimiste. La façon de couper la circulation routière grâce à de l'essence enflammée est une bonne idée qui aurait pu être beaucoup plus spectaculaire même en 1962. Tout est un peu comme cela, tout ce qui est montré manque d'éclat , reste superficiel et ne nous touche pas assez.\",\n",
       " \"le film est très mauvais car : trop peu d'actions effets spéciaux non tape-a-l’œil le perso principal est creux le jeu d'acteur n'est pas geniale toutefois le scenario est bon ( dans la norme ) je vous deconseile de le voir sauf si malheuresement vous voulez perdre votre temps\",\n",
       " \"aie aie aie! En corse ça passait encore! Mais là c'est encore plus déprimant que saint-Tropez en hiver. Et c'est peu de le dire! Allez randonner plutôt que de perdre votre temps devant cette suite qui ne s'imposait vraiment pas.\",\n",
       " \"Je ne me lancerai pas dans une analyse fine sur la réhabilitation de la banlieue au cinéma, ni sur les clichés sur l'univers du cinéma véhiculés par ce film, simplement parce que je trouve ce film dramatiquement plat et que d'autres l'ont déjà fait pour moi... Le film est juste tristement ennuyeux.\",\n",
       " '\"Subway\":2ème réalisation de Luc Besson.Une plongée dans les méandres du métro parisien.Très avant-gardiste en 1985(musique,looks),Subway se regarde comme un tableau néo-moderne plus que comme un film au sens classique du terme.C\\'est plus cadré que ça en a l\\'air.Personnellement je n\\'ai pas adhéré,certaines scènes ont drolement vieillies.J\\'ai tout de meme aimé les personnages de Bacri et Galabru.Lambert est fade,Adjani agacante.Subway pose les jalons des futurs oeuvres de Besson,et semble inabouti scénaristiquement contrairement à l\\'esthétique pub.Pour les fans du réalisateur uniquement.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples = raw_datasets[\"train\"].shuffle(seed=42).filter(lambda example: example['label']==0)\n",
    "positive_samples['review'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing du dataset\n",
    "\n",
    "Pour l'entraînement du modèle de classification, on utiliser un `Trainer` de HuggingFace. Il utilise un dataset comportant 2 features :\n",
    "- le label (0 ou 1)\n",
    "- les input_ids tokenisés\n",
    "- les attention_mask (1 si le mot est présent, 0 sinon)\n",
    "\n",
    "On va donc pré-processer notre dataset avec le Tokeniser du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb/cache-640662018a3bdca4.arrow\n",
      "100%|██████████| 20/20 [00:06<00:00,  2.96ba/s]\n",
      "Loading cached processed dataset at /Users/herve/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/91f700d606838c22c5c370846746e60503219d0c1f16ed96bfd1fa19a73458eb/cache-8fc7790614434712.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (160000, 4), 'validation': (20000, 4), 'test': (20000, 4)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5332e541e6d47f8eaf92d62b92f50e94dc4471375500c1337eb2fe711fbfc8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
